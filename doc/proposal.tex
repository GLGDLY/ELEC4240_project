% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\begin{document}

%%%%%%%%% TITLE 
\title{Project proposal for Text Eraser on Image}

\author{
Members: Leung Tsz Kit Gary (tkleungal, 20863513);  % Fill in name here
Ip Marisa (mip, 21054353)
}
\maketitle

%%%%%%%%% CONTENT 
\section{Background}

In this project, we aim to develop a model solution to perform the task of text removal on images. 
The idea was initiated from the problem that most of the text eraser solutions online currently are performed with English word processing, 
with bad performance on processing of Chinese characters mixed with English characters. 
However, in the Hong Kong environment, such a case is a common scenario. Therefore, we would like to develop a solution that can handle such use case.

\section{Proposed Methodology}

\subsection{Implementation}

It is proposed that the machine learning solution will be split into 2 parts:
\begin{enumerate}
    \item Image text segmentation
    \item Inpainting
\end{enumerate}

For the flow of the solution, a network of image text segmentation will first be computed with the input image.
The output of the segmentation will then be a mask of the text region.
This mask can then be used for the inpainting model to remove the text region from the image.
Resulting in the final output image with the text removed.

\subsubsection{Image text segmentation}

% https://github.com/FudanVI/benchmarking-chinese-text-recognition
% https://github.com/SHI-Labs/Rethinking-Text-Segmentation
% https://paperswithcode.com/dataset/icdar-2013
% https://paperswithcode.com/dataset/rctw-17
% https://paperswithcode.com/dataset/msda

For the image text segmentation task, we propose using an adapted U-Net architecture, a fully convolutional neural network architecture 
developed for image segmentation, with further improvement to address mixed Chinese and English text detection, given its ability to capture 
both contextual information and fine details through skip connections. 
The U-Net model will be trained to recognize and separate text regions from images with stronger feature extraction capacity by network depth 
adjustment and use of the attention mechanism. Improvements to the encoder and decoder path are also under consideration, 
such as the use of ResNet blocks on encoders to learn text features in various scales, and the strengthening of skip connections to preserve fine spatial details respectively.

Regarding the training dataset selection, we plan to apply existing text recognition datasets in an attempt to achieve good performance 
across diverse text styles and languages. For instance, we will apply the MSDA and RCTW-17 datasets to provide a range of pre-annotated images 
with multi-language text in diverse styles and complex backgrounds. 
Besides, we will implement strong data augmentation techniques to better enable the model to generalize. This includes random rotations, 
scale variations, and contrast adjustments to simulate different text appearances. 
By training on both real and simulated data, we aim to improve the ability of the model to accurately segment text in challenging images, 
thus producing a stable mask for the inpainting process to work with.

\subsubsection{Inpainting}

% https://huggingface.co/docs/diffusers/v0.25.0/using-diffusers/inpaint

For the implementation of the inpainting task, variants of U-Net with skip connections can be used to preserve spatial and high-level 
details of the input image for the inpainting model. Also, the Partial Convolution Layer from Nvidia 
(https://github.com/NVIDIA/partialconv) can be used to replace the standard convolution layer and carry out the inpainting task. To 
generate more realistic results, GAN-based architecture can also be considered to combine the inpainting model for better performance.
Therefore, with the above requirements, we are currently investigating the possibility of using the pix2pix architecture,
which is a conditional GAN model that can be used for image-to-image translation tasks with its generator as a modified U-Net, 
with modifying its generator to use the Partial Convolution Layer for achieving the inpainting task.

For the dataset used in training, first, we are planning to find some text detection-related online datasets, 
such as the COCO-Text dataset, which filters out the images that contain no text for putting into our training dataset. 
Also, we are also actively investigating the possibility of using image generation models to generate images that contain 
no text as the training data for our inpainting model. These image data will then be used as the ground truth of our training dataset, 
while the actual training images will be generated by overlaying several white rectangle masks with random size and position on the original image. 
We believe that our model can learn the features of these images that contain no text and use them for generating the filling of the text region.

\subsection{Evaluation}

TensorBoard will be used to monitor the model's training process, where the required metrics will be displayed as graphs and charts 
for us to monitor the training process. The evaluation will be carried out separately for the segmentation model and the inpainting 
model, correspondingly to the below metrics:

For the segmentation model:
\begin{enumerate}
    \item $IoU$: Intersection of Union of the predicted mask and the ground truth mask, should be maximized
    \item $F1\;score$: The harmonic mean of precision and recall, should be maximized
\end{enumerate}

For the inpainting model:
\begin{enumerate}
    \item $gen\_gan\_loss$: The loss of the generator in the GAN model, should be balanced with the $disc\_loss$
    \item $disc\_loss$: The loss of the discriminator in the GAN model, should be balanced with the $gen\_gan\_loss$
    \item $gen\_l1\_loss$: Mean absolute error between the generated image and the ground truth, should be minimized
\end{enumerate}

\end{document}
