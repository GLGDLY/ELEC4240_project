% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
% \usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}


% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


\begin{document}

%%%%%%%%% TITLE 
\title{Project proposal for Text Eraser on Image}

\author{
Members: Leung Tsz Kit Gary (tkleungal, 20863513);  % Fill in name here
Ip Marisa (mip, 21054353)
}
\maketitle

%%%%%%%%% CONTENT 
\section{Background}

In this project, we aim to develop a model solution to perform the task of text removal on images. 
The idea was initiated from the problem that most of the text eraser solutions online currently are performed with English word processing, 
with bad performance on processing of Chinese characters mixed with English characters. 
However, in the Hong Kong environment, such case is a common scenario. Therefore, we would like to develop a solution that can handle such use case.

\section{Proposed Methodology}

\subsection{Implementation}

It is proposed that the machine learning solution will be split into 2 parts:
\begin{enumerate}
    \item Image text segmentation
    \item Inpainting
\end{enumerate}

For the flow of the solution, a network of image text segmentation will first be computed with the input image.
The output of the segmentation will then be a mask of the text region.
This mask can then be used for the inpainting model to remove the text region from the image.
Resulting in the final output image with the text removed.

\subsubsection{Image text segmentation}

% https://github.com/FudanVI/benchmarking-chinese-text-recognition
% https://github.com/SHI-Labs/Rethinking-Text-Segmentation
% https://paperswithcode.com/dataset/icdar-2013
% https://paperswithcode.com/dataset/rctw-17
% https://paperswithcode.com/dataset/msda

\subsubsection{Inpainting}

% https://huggingface.co/docs/diffusers/v0.25.0/using-diffusers/inpaint

For the implementation of the inpainting task, variants of U-Net with skip connections can be used to preserve spatial and high-level 
details of the input image for the inpainting model. Also, the Partial Convolution Layer from Nvidia 
(https://github.com/NVIDIA/partialconv) can be used to replace the standard convolution layer and carry out the inpainting task. To 
generate more realistic results, GAN-based architecture can also be considered to combine the inpainting model for better performance.
Therefore, with the above requirements, we are currently investigating the possibility of using the pix2pix architecture,
which is a conditional GAN model that can be used for image-to-image translation tasks with its generator as a modified U-Net, 
with modifying its generator to use the Partial Convolution Layer for achieving the inpainting task.

For the dataset used in training, first, we are planning to find some text detection-related online datasets, 
such as the COCO-Text dataset, which filters out the images that contain no text for putting into our training dataset. 
Also, we are also actively investigating the possibility of using image generation models to generate images that contain 
no text as the training data for our inpainting model. These image data will then be used as the ground truth of our training dataset, 
while the actual training images will be generated by overlaying several white rectangle masks with random size and position on the original image. 
We believe that our model can learn the features of these images that contain no text and use them for generating the filling of the text region.

\subsection{Evaluation}

% TensorBoard?

\end{document}
